{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from ../definitions/definitions_EDA\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "sys.path.append('../definitions')\n",
    "import definitions_EDA as eda\n",
    "# import definitions_plotting as def_plot\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from scipy import signal\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from scipy.signal import find_peaks\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2024-03-22')\n",
    "end_date = pd.Timestamp('2024-03-23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data and store as dataframe (data ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-23 00:00:00\n",
      "2024-03-24 00:00:00\n",
      "                          Time  NS_SQUID   Z_SQUID  NS_Fluxgate  EW_Fluxgate  \\\n",
      "0      2024-03-22 00:00:00.200  -21.3009   -7.2962   10926.4116     -85.9870   \n",
      "161998 2024-03-23 01:00:00.030   25.9159    3.2965   10958.5503    -103.7426   \n",
      "314397 2024-03-24 00:00:00.040   -5.4883  -55.1467   10937.7543     -65.4179   \n",
      "431996 2024-03-23 00:00:00.000  -57.9540 -216.6023   10952.3433     -71.7599   \n",
      "593995 2024-03-24 01:00:00.010  -77.5388 -235.5826   10934.8187     -92.8163   \n",
      "746394 2024-03-25 00:00:00.010  -93.0866 -236.0817   10923.9636     -66.6165   \n",
      "863992 2024-03-26 17:32:39.750  -83.3584 -187.5898   10931.4587     -72.0992   \n",
      "\n",
      "        Z_Fluxgate        Date  Date_change  \n",
      "0      -22660.2847  2024-03-22         True  \n",
      "161998 -22650.9890  2024-03-23         True  \n",
      "314397 -22673.9400  2024-03-24         True  \n",
      "431996 -22670.8357  2024-03-23         True  \n",
      "593995 -22684.3487  2024-03-24         True  \n",
      "746394 -22685.3896  2024-03-25         True  \n",
      "863992 -22645.6725  2024-03-26         True  \n",
      "The total number of duplicates is: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot infer freq from a non-convertible index of dtype int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_arr_mag \u001b[38;5;241m=\u001b[39m eda\u001b[38;5;241m.\u001b[39mprocess_data(eda\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctumag\u001b[39m\u001b[38;5;124m'\u001b[39m, eda\u001b[38;5;241m.\u001b[39mread_txt_file, start_date, end_date))\n\u001b[1;32m      2\u001b[0m data_arr_squid \u001b[38;5;241m=\u001b[39m eda\u001b[38;5;241m.\u001b[39mprocess_data(eda\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquid\u001b[39m\u001b[38;5;124m'\u001b[39m, eda\u001b[38;5;241m.\u001b[39mread_txt_file, start_date, end_date))\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43meda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_arr_mag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_arr_squid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mbetween_time(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m23:59:50\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00:00:10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Code/Bigdata_Event_Detection/bigdata_detection/modules/../definitions/definitions_EDA.py:128\u001b[0m, in \u001b[0;36mcreate_dataframe\u001b[0;34m(data_arr_mag, data_arr_squid, start_date)\u001b[0m\n\u001b[1;32m    125\u001b[0m     df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta(df\u001b[38;5;241m.\u001b[39mgroupby(df\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mcumcount(), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Optionally, infer the frequency of the time series and set it\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m inferred_freq \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_freq:\n\u001b[1;32m    130\u001b[0m     df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;241m=\u001b[39m inferred_freq\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/tseries/frequencies.py:148\u001b[0m, in \u001b[0;36minfer_freq\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inferer\u001b[38;5;241m.\u001b[39mget_freq()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_numeric_dtype(index\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot infer freq from a non-convertible index of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, DatetimeIndex):\n\u001b[1;32m    153\u001b[0m     index \u001b[38;5;241m=\u001b[39m DatetimeIndex(index)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot infer freq from a non-convertible index of dtype int64"
     ]
    }
   ],
   "source": [
    "data_arr_mag = eda.process_data(eda.get_data('ctumag', eda.read_txt_file, start_date, end_date))\n",
    "data_arr_squid = eda.process_data(eda.get_data('squid', eda.read_txt_file, start_date, end_date))\n",
    "df = eda.create_dataframe(data_arr_mag, data_arr_squid, start_date)\n",
    "df = df.between_time('23:59:50', '00:00:10')\n",
    "print(df)\n",
    "df = df.astype({'NS_SQUID': 'float32', 'Z_SQUID': 'float32', 'NS_Fluxgate': 'float32', 'EW_Fluxgate': 'float32', 'Z_Fluxgate': 'float32'})\n",
    "del data_arr_mag, data_arr_squid\n",
    "gc.collect()\n",
    "observations_per_day = df.resample('D').size()\n",
    "print(observations_per_day)\n",
    "# df = df.between_time('12:30:00', '13:30:00') # Select only a small subset of the data for analysis\n",
    "# print(f' \\n Shape of df', df.shape)\n",
    "# print(f' \\nNumber of days data = Total records {df.shape[0]} / records per day (431998) = {df.shape[0]/431998} ')\n",
    "# print(f' \\nHead of dataframe: \\n', df.head().to_string(index=True))\n",
    "eda.generateDataPlots(df['NS_SQUID'].values, df['Z_SQUID'].values, df['NS_Fluxgate'].values, df['EW_Fluxgate'].values, df['Z_Fluxgate'].values, df.shape[0], 431997, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale SQUID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the Offset (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Quiet Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_quiet = '2024-03-26'\n",
    "end_date_quiet = '2024-03-31'\n",
    "\n",
    "data_arr_mag = eda.process_data(eda.get_data('ctumag', eda.read_txt_file, start_date_quiet, end_date_quiet))\n",
    "data_arr_squid = eda.process_data(eda.get_data('squid', eda.read_txt_file, start_date_quiet, end_date_quiet))\n",
    "df_quiet = eda.create_dataframe(data_arr_mag, data_arr_squid, start_date_quiet)\n",
    "df_quiet = df_quiet.astype({'NS_SQUID': 'float32', 'Z_SQUID': 'float32', 'NS_Fluxgate': 'float32', 'EW_Fluxgate': 'float32', 'Z_Fluxgate': 'float32'})\n",
    "del data_arr_mag, data_arr_squid\n",
    "gc.collect()\n",
    "# df = df.between_time('12:30:00', '13:30:00') # Select only a small subset of the data for analysis\n",
    "# print(f' \\n Shape of df', df.shape)\n",
    "print(f' \\nNumber of days data = Total records {df_quiet.shape[0]} / records per day (431998) = {df_quiet.shape[0]/431998} ')\n",
    "print(f' \\nHead of dataframe: \\n', df_quiet.head().to_string(index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = [\"NS_SQUID\",\"Z_SQUID\"]\n",
    "for column in df_quiet.columns:\n",
    "    feature_mean = df_quiet[column].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "\n",
    "print(f'New head after standardize', df.head())\n",
    "print(f\"Shape after normalisation\", df.shape)\n",
    "eda.generateDataPlots(df['NS_SQUID'].values, df['Z_SQUID'].values, df['NS_Fluxgate'].values, df['EW_Fluxgate'].values, df['Z_Fluxgate'].values, df.shape[0], 431997, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values and zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"\\n Number of missing values in {column} is: \", df[column].isna().sum())\n",
    "    print(f\"Nmber of zeros in {column} is: \", (df[column] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z score test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' \\n Shape of df before removing outliers', df.shape)\n",
    "outliers_removed_z = eda.z_score_test(df)\n",
    "print(f' \\n Shape of df after removing outliers', outliers_removed_z.shape)\n",
    "print(f'\\nProportion of outliers removed:', (1-outliers_removed_z.shape[0]/df.shape[0])*100, '%')\n",
    "del df\n",
    "gc.collect()\n",
    "eda.generateDataPlots(outliers_removed_z['NS_SQUID'].values, outliers_removed_z['Z_SQUID'].values, outliers_removed_z['NS_Fluxgate'].values, outliers_removed_z['EW_Fluxgate'].values, outliers_removed_z['Z_Fluxgate'].values, outliers_removed_z.shape[0], 431997, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix sudden jumps or drops in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are severe drops in the squid data. Lets fix these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_df_ns = eda.detect_spikes_and_correct(outliers_removed_z, \"NS_SQUID\")\n",
    "corrected_df_f = eda.detect_spikes_and_correct(corrected_df_ns, \"Z_SQUID\")\n",
    "corrected_df_nf = eda.detect_spikes_and_correct(corrected_df_f, \"NS_Fluxgate\")\n",
    "corrected_df_ew = eda.detect_spikes_and_correct(corrected_df_nf, \"EW_Fluxgate\")\n",
    "corrected_df = eda.detect_spikes_and_correct(corrected_df_ew, \"Z_Fluxgate\")\n",
    "del outliers_removed_z, corrected_df_ns, corrected_df_f, corrected_df_nf, corrected_df_ew\n",
    "gc.collect()\n",
    "eda.generateDataPlots(corrected_df['NS_SQUID'].values, corrected_df['Z_SQUID'].values, corrected_df['NS_Fluxgate'].values, corrected_df['EW_Fluxgate'].values, corrected_df['Z_Fluxgate'].values, corrected_df.shape[0], 431997, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the H component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_component = np.sqrt(corrected_df['NS_Fluxgate']**2 + corrected_df['EW_Fluxgate']**2)\n",
    "corrected_df.loc[:,\"H Component\"] = h_component\n",
    "# print(corrected_df)\n",
    "observations_per_day = corrected_df.resample('D').size()\n",
    "print(observations_per_day)\n",
    "# print(f\"Shape after feature generation:\", corrected_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_time_series(df, start_date, end_date):\n",
    "  \"\"\"Resamples time series data from 5Hz to 1 sample per minute for a given date range.\n",
    "\n",
    "  Args:\n",
    "    df: Pandas DataFrame containing the time series data.\n",
    "    start_date: Start date for resampling.\n",
    "    end_date: End date for resampling.\n",
    "\n",
    "  Returns:\n",
    "    Resampled Pandas DataFrame with 1 sample per minute.\n",
    "  \"\"\"\n",
    "\n",
    "  # Filter data for the specified date range\n",
    "  filtered_df = df[(df.index >= start_date) & (df.index < end_date)]\n",
    "  print(f\"This is the size of the df for day \", start_date,\" has shape: \", filtered_df.shape)\n",
    "  # Resample to 1 minute frequency\n",
    "  resampled_df = filtered_df.resample('min').mean()  # Adjust resampling method as needed\n",
    "\n",
    "  return resampled_df\n",
    "\n",
    "resampled_df = pd.DataFrame()\n",
    "# Resample data for each day\n",
    "for day in pd.date_range(start_date, end_date, freq='D'):\n",
    "  resampled_data = resample_time_series(corrected_df, day, day + pd.Timedelta(days=1))\n",
    "  # Do something with the resampled data, e.g., save to a file\n",
    "  resampled_data.to_csv(f'/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Data/MIN DATA/{day.strftime(\"%Y-%m-%d\")}.csv')\n",
    "  print(f\"The shape of day \",day, \" in the data is: \", resampled_data.shape)\n",
    "  resampled_df = pd.concat([resampled_df, resampled_data])\n",
    "\n",
    "print(f\"This is the new resampled dataframe\\n\", resampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.perform_dickey_fuller_test(resampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for seasonality and trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.test_stationarity(corrected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "for column in resampled_df.columns:\n",
    "    plt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})\n",
    "    autocorrelation_plot(resampled_df[column].tolist())\n",
    "    plt.title(f'Autocorrelation for {column}')\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components, fourier_results = eda.calculate_fourier_transforms(df)\n",
    "eda.plot_fourier_transform(fourier_results, components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PDF of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for column in resampled_df.columns:\n",
    "    sns.displot(resampled_df[column], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "# result_add = seasonal_decompose(df['Z_Fluxgate'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "# plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "# result_add.plot().suptitle('Additive Decompose', fontsize=22)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the preprocessed dataframe to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df.to_csv('/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Data/RESAMPLED/{}-{}.csv'.format(start_date, end_date), index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
