{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from ../definitions/definitions_EDA\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../definitions')\n",
    "import definitions_EDA as eda\n",
    "# import definitions_plotting as def_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import gc\n",
    "import scipy as sp\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "\n",
    "RECORDS_PER_DAY = 431996\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Code/Bigdata_Event_Detection/bigdata_detection/modules/../definitions/definitions_EDA.py:18: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(StringIO(data), sep = \"\t\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m eda\u001b[38;5;241m.\u001b[39mcalculate_h_component(df)\n\u001b[1;32m     16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43meda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_mean_of_five_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Data/NOT STORM/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Code/Bigdata_Event_Detection/bigdata_detection/modules/../definitions/definitions_EDA.py:689\u001b[0m, in \u001b[0;36mcalculate_mean_of_five_in_chunks\u001b[0;34m(df, chunk_size, step_size)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# Iterate through the chunk in steps of step_size (e.g., 15)\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_chunk), step_size):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Get the mean of the current group of step_size numbers\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     group_mean \u001b[38;5;241m=\u001b[39m \u001b[43mdf_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# Append the mean to the list for the current column\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     column_means\u001b[38;5;241m.\u001b[39mappend(group_mean)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/series.py:6540\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6532\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6538\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6539\u001b[0m ):\n\u001b[0;32m-> 6540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/generic.py:12417\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12412\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12415\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12418\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/generic.py:12374\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12370\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12372\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/series.py:6448\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6443\u001b[0m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[1;32m   6444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6447\u001b[0m     )\n\u001b[0;32m-> 6448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "start_date = pd.Timestamp('2024-03-20')\n",
    "end_date = pd.Timestamp('2024-07-10')\n",
    "# List of specific dates\n",
    "storm_dates = ['2024-05-11', '2024-05-10', '2024-08-12', '2024-05-12', '2024-03-24',\n",
    "             '2024-08-13', '2024-04-19', '2024-08-04', '2024-03-03', '2024-06-28', \n",
    "             '2024-05-13', '2024-05-17', '2024-05-02', '2024-05-18', '2024-05-16', \n",
    "             '2024-03-25', '2024-03-04']\n",
    "date_list = pd.to_datetime(storm_dates)\n",
    "\n",
    "for day in pd.date_range(start_date, end_date, freq='D'):\n",
    "    if day not in date_list:\n",
    "        start_date = day.date()\n",
    "        end_date = day.date()\n",
    "        df = eda.get_data(eda.read_txt_file, start_date, end_date)\n",
    "        df = eda.calculate_h_component(df)\n",
    "        df['flag'] = 0\n",
    "        df = eda.calculate_mean_of_five_in_chunks(df)\n",
    "        df.to_csv(f'/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Data/NOT STORM/{start_date.strftime(\"%Y-%m-%d\")}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data and store as dataframe (data ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eda.get_data(eda.read_txt_file, start_date, end_date)\n",
    "observations_per_day = df.resample('D').size()\n",
    "#df = df.iloc[350000:863994, :] # Select some subset of data\n",
    "# df = df.astype({'NS_SQUID': 'float32', 'Z_SQUID': 'float32', 'NS_Fluxgate': 'float32', 'EW_Fluxgate': 'float32', 'Z_Fluxgate': 'float32'})\n",
    "print(f\"Number of observations per day: \\n\",observations_per_day)\n",
    "print(f' \\n Shape of dataframe', df.shape)\n",
    "print(f' \\nNumber of days data = Total records {df.shape[0]} / records per day {RECORDS_PER_DAY} = {df.shape[0]/RECORDS_PER_DAY} ')\n",
    "print(f' \\nHead of dataframe: \\n', df.head().to_string(index=True))\n",
    "eda.generateDataPlots(df['NS_SQUID'].values, df['Z_SQUID'].values, df['NS_Fluxgate'].values, df['EW_Fluxgate'].values, df['Z_Fluxgate'].values, len(df), RECORDS_PER_DAY, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale SQUID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the Offset (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Quiet Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date_quiet = '2024-03-26'\n",
    "# end_date_quiet = '2024-03-31'\n",
    "\n",
    "# data_arr_mag = eda.process_data(eda.get_data('ctumag', eda.read_txt_file, start_date_quiet, end_date_quiet))\n",
    "# data_arr_squid = eda.process_data(eda.get_data('squid', eda.read_txt_file, start_date_quiet, end_date_quiet))\n",
    "# df_quiet = eda.create_dataframe(data_arr_mag, data_arr_squid, start_date_quiet)\n",
    "# df_quiet = df_quiet.astype({'NS_SQUID': 'float32', 'Z_SQUID': 'float32', 'NS_Fluxgate': 'float32', 'EW_Fluxgate': 'float32', 'Z_Fluxgate': 'float32'})\n",
    "# del data_arr_mag, data_arr_squid\n",
    "# gc.collect()\n",
    "# # df = df.between_time('12:30:00', '13:30:00') # Select only a small subset of the data for analysis\n",
    "# # print(f' \\n Shape of df', df.shape)\n",
    "# print(f' \\nNumber of days data = Total records {df_quiet.shape[0]} / records per day (431998) = {df_quiet.shape[0]/431998} ')\n",
    "# print(f' \\nHead of dataframe: \\n', df_quiet.head().to_string(index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns  = [\"NS_SQUID\",\"Z_SQUID\"]\n",
    "# for column in df_quiet.columns:\n",
    "#     feature_mean = df_quiet[column].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in df.columns:\n",
    "#     df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "\n",
    "# print(f'New head after standardize\\n', df.head())\n",
    "# print(f\"Shape after normalisation\", df.shape)\n",
    "# eda.generateDataPlots(df['NS_SQUID'].values, df['Z_SQUID'].values, df['NS_Fluxgate'].values, df['EW_Fluxgate'].values, df['Z_Fluxgate'].values, len(df), RECORDS_PER_DAY, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values and zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"\\n Number of missing values in {column} is: \", df[column].isna().sum())\n",
    "    print(f\"Nmber of zeros in {column} is: \", (df[column] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z score test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f' \\n Shape of df before removing outliers', df.shape)\n",
    "# outliers_removed_z = eda.z_score_test(df)\n",
    "# print(f\"Head of outliers removed dataframe:\\n\", outliers_removed_z.head())\n",
    "# print(f' \\n Shape of df after removing outliers', outliers_removed_z.shape)\n",
    "# print(f'\\nProportion of data removed:', (1-outliers_removed_z.shape[0]/df.shape[0])*100, '%')\n",
    "# del df\n",
    "# gc.collect()\n",
    "# observations_per_day = outliers_removed_z.resample('D').size()\n",
    "# print(observations_per_day)\n",
    "# eda.generateDataPlots(outliers_removed_z['NS_SQUID'].values, outliers_removed_z['Z_SQUID'].values, outliers_removed_z['NS_Fluxgate'].values, outliers_removed_z['EW_Fluxgate'].values, outliers_removed_z['Z_Fluxgate'].values,len(outliers_removed_z), RECORDS_PER_DAY, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix sudden jumps or drops in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are severe drops in the squid data. Lets fix these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_df_ns = eda.detect_spikes_and_correct(outliers_removed_z, \"NS_SQUID\")\n",
    "# corrected_df_f = eda.detect_spikes_and_correct(corrected_df_ns, \"Z_SQUID\")\n",
    "# corrected_df_nf = eda.detect_spikes_and_correct(corrected_df_f, \"NS_Fluxgate\")\n",
    "# corrected_df_ew = eda.detect_spikes_and_correct(corrected_df_nf, \"EW_Fluxgate\")\n",
    "# corrected_df = eda.detect_spikes_and_correct(corrected_df_ew, \"Z_Fluxgate\")\n",
    "# del outliers_removed_z, corrected_df_ns, corrected_df_f, corrected_df_nf, corrected_df_ew\n",
    "# gc.collect()\n",
    "# eda.generateDataPlots(corrected_df['NS_SQUID'].values, corrected_df['Z_SQUID'].values, corrected_df['NS_Fluxgate'].values, corrected_df['EW_Fluxgate'].values, corrected_df['Z_Fluxgate'].values, len(corrected_df), RECORDS_PER_DAY, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the H component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corrected_df = eda.calculate_h_component(df)\n",
    "# print(corrected_df)\n",
    "observations_per_day = corrected_df.resample('D').size()\n",
    "print(observations_per_day)\n",
    "# print(f\"Shape after feature generation:\", corrected_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make the problem supervised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "END = 240000\n",
    "test = df.iloc[START:END, :] # Select some subset of data\n",
    "eda.generateDataPlots(test['NS_SQUID'].values, test['Z_SQUID'].values, test['NS_Fluxgate'].values, test['EW_Fluxgate'].values, test['Z_Fluxgate'].values, len(test), RECORDS_PER_DAY, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific dates\n",
    "storm_dates = ['2024-05-11', '2024-05-10', '2024-08-12', '2024-05-12', '2024-03-24', \n",
    "             '2024-08-13', '2024-04-19', '2024-08-04', '2024-03-03', '2024-06-28', \n",
    "             '2024-05-13', '2024-05-17', '2024-05-02', '2024-05-18', '2024-05-16', \n",
    "             '2024-03-25', '2024-03-04']\n",
    "date_list = pd.to_datetime(storm_dates)\n",
    "\n",
    "\n",
    "\n",
    "# corrected_df['Date'] = pd.to_datetime(corrected_df.index.date)\n",
    "\n",
    "corrected_df['flag'] = np.where((corrected_df.index >= corrected_df.index[START]) & (corrected_df.index <= corrected_df.index[END]), 1, 0)\n",
    "# corrected_df['flag'] = corrected_df['Date'].isin(date_list).astype(int)\n",
    "# corrected_df = corrected_df.drop('Date', axis=1)\n",
    "\n",
    "print(corrected_df.head())\n",
    "print(corrected_df.shape)\n",
    "num_ones = corrected_df['flag'].sum()\n",
    "print(num_ones/len(corrected_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(corrected_df['NS_SQUID'], label='NS_SQUID')\n",
    "plt.scatter(corrected_df[corrected_df['flag'] == 1].index, corrected_df[corrected_df['flag'] == 1]['NS_SQUID'], color='red', label='1s')\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('NS_SQUID')\n",
    "plt.title('NS_SQUID with 1s')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "eda.generateDataPlots(corrected_df['NS_SQUID'].values, corrected_df['Z_SQUID'].values, corrected_df['NS_Fluxgate'].values, corrected_df['EW_Fluxgate'].values, corrected_df['Z_Fluxgate'].values, len(corrected_df), RECORDS_PER_DAY, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df = pd.DataFrame()\n",
    "print(f\"\\nThis is the shape of the original data\\n\", corrected_df.shape)\n",
    "resampled_data = eda.calculate_mean_of_five_in_chunks(corrected_df)\n",
    "resampled_data.to_csv(f'/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Data/STORM LABELLED/{start_date.strftime(\"%Y-%m-%d\")}.csv')\n",
    "# Resample data for each day\n",
    "# for day in pd.date_range(start_date, end_date, freq='D'):\n",
    "#     resampled_data = eda.resample_time_series(corrected_df, day, day + pd.Timedelta(days=1)).dropna()\n",
    "#     resampled_data.to_csv(f'/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester 2/Skripsie/Data/MIN DATA/{day.strftime(\"%Y-%m-%d\")}.csv')\n",
    "#     resampled_df = pd.concat([resampled_df, resampled_data])  \n",
    "\n",
    "\n",
    "print(f\"\\nThis is the shape of the resampled data\\n\", resampled_data.shape)\n",
    "print(f\"\\nThis is the new resampled dataframe\\n\", resampled_data.head())\n",
    "#eda.generateDataPlots(resampled_df['NS_SQUID'].values, resampled_df['Z_SQUID'].values, resampled_df['NS_Fluxgate'].values, resampled_df['EW_Fluxgate'].values, resampled_df['Z_Fluxgate'].values, len(resampled_df), RECORDS_PER_DAY, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.perform_dickey_fuller_test(resampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for seasonality and trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.test_stationarity(corrected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "for column in resampled_df.columns:\n",
    "    plt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})\n",
    "    autocorrelation_plot(resampled_df[column].tolist())\n",
    "    plt.title(f'Autocorrelation for {column}')\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components, fourier_results = eda.calculate_fourier_transforms(df)\n",
    "eda.plot_fourier_transform(fourier_results, components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PDF of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for column in resampled_df.columns:\n",
    "    sns.displot(resampled_df[column], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "# result_add = seasonal_decompose(df['Z_Fluxgate'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "# plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "# result_add.plot().suptitle('Additive Decompose', fontsize=22)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a signal 'sig' sampled at regular intervals ('time_step')\n",
    "sig = resampled_data[\"H Component\"].to_numpy()\n",
    "sig_fft = sp.fft.fft(sig)\n",
    "power = np.abs(sig_fft) ** 2\n",
    "sample_freq = sp.fft.fftfreq(sig.size, d=1)\n",
    "\n",
    "# Plot the FFT power\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(sample_freq, power)\n",
    "plt.xlabel(\"Frequency [Hz]\")\n",
    "plt.ylabel(\"Power\")\n",
    "\n",
    "# Find the peak frequency\n",
    "pos_mask = np.where(sample_freq > 0)\n",
    "freqs = sample_freq[pos_mask]\n",
    "peak_freq = freqs[power[pos_mask].argmax()]\n",
    "\n",
    "# An inner plot to show the peak frequency\n",
    "axes = plt.axes([0.55, 0.3, 0.3, 0.5])\n",
    "plt.title(\"Peak frequency\")\n",
    "plt.plot(freqs[:8], power[pos_mask][:8])\n",
    "plt.setp(axes, yticks=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resampled_df)\n",
    "# Function to plot waterfall FFT for each component\n",
    "def plot_waterfall(df, fs=1/60):\n",
    "    nfft = 256  # Length of each segment\n",
    "    noverlap = 128  # Overlap between segments\n",
    "\n",
    "    for column in df.columns:\n",
    "        signal = df[column].values\n",
    "        plt.figure()\n",
    "        plt.specgram(signal, NFFT=nfft, Fs=fs, noverlap=noverlap, cmap='viridis',norm=LogNorm())\n",
    "        plt.title(f'Waterfall FFT Plot for {column}')\n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.ylabel('Frequency [Hz]')\n",
    "        plt.colorbar(label='Intensity [dB]')\n",
    "        plt.show()\n",
    "\n",
    "# Plot the waterfall FFT for each component\n",
    "plot_waterfall(resampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
