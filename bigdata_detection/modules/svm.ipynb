{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester_2/Skripsie/Code/Bigdata_Event_Detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../definitions')\n",
    "import def_model as mod_def\n",
    "import definitions_EDA as eda\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, TimeSeriesSplit,RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.fft import fft, rfftfreq\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_genetic import GASearchCV\n",
    "from sklearn_genetic.space import Continuous, Categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2024-03-20')\n",
    "end_date = pd.Timestamp('2024-07-10')\n",
    "n_lags = 5\n",
    "shift_steps = 900  # corresponds to 3 hours (1 record every 12 seconds)\n",
    "\n",
    "# Optimize the hyperparameter search space to reduce computation\n",
    "param_grid = {\n",
    "    \"C\": Continuous(1e-2, 10, distribution=\"log-uniform\"),  # Narrow the range for C\n",
    "    \"kernel\": Categorical(['linear', 'rbf']),  # Only two kernel types to speed up search\n",
    "    \"gamma\": Continuous(1e-4, 0.1, distribution=\"log-uniform\"),  # Reduce the search range for gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the head of the df: \n",
      "     NS_SQUID   Z_SQUID   NS_Fluxgate  EW_Fluxgate    Z_Fluxgate   H Component  \\\n",
      "0 -17.136570  0.276270  10934.364450   -42.370033 -22656.488219  10934.446542   \n",
      "1 -17.113152  0.350281  10934.410213   -42.314737 -22656.308095  10934.492090   \n",
      "2 -17.235085  0.185658  10934.400975   -42.445483 -22656.408618  10934.483360   \n",
      "3 -17.030711 -0.242669  10934.482125   -42.588208 -22656.805620  10934.565063   \n",
      "4 -16.934697 -0.295667  10934.616765   -42.609385 -22656.955439  10934.699784   \n",
      "\n",
      "   flag  \n",
      "0   0.0  \n",
      "1   0.0  \n",
      "2   0.0  \n",
      "3   0.0  \n",
      "4   0.0  \n",
      "This is the shape: (163594, 7)\n",
      "Flag = 1 counts: 8590\n",
      "Proportion of flag = 1 to flag = 0: 5.542328810431709 %\n"
     ]
    }
   ],
   "source": [
    "df = mod_def.combine_resampled_data(start_date, end_date)\n",
    "# let df = subset\n",
    "# df = df.iloc[:, 5:7]\n",
    "# downsample\n",
    "df_resampled = eda.calculate_mean_of_five_in_chunks(df,1000, 5)\n",
    "df = df_resampled\n",
    "\n",
    "print(f\"This is the head of the df: \\n\", df.head())\n",
    "print(f\"This is the shape:\" ,df.shape)\n",
    "flag_counts = df['flag'].value_counts()\n",
    "proportion_flag_1_to_0 = (flag_counts[1.0] / flag_counts[0.0])*100\n",
    "print(f\"Flag = 1 counts: {flag_counts[1.0]}\")\n",
    "print(f\"Proportion of flag = 1 to flag = 0: {proportion_flag_1_to_0} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example multivariate time series data (e.g., 3 variables) - highly imbalanced\n",
    "np.random.seed(42)\n",
    "n_samples = 2000  # Simulate 2000 samples for demonstration (about 6.6 hours)\n",
    "n_features = 3\n",
    "data = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Introduce imbalance (90% 0s and 10% 1s)\n",
    "data[-200:, -1] = 1  # artificially make 10% of last column `1` to simulate imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape of X (162689, 41)\n",
      "This is the shape of Y (162689,)\n"
     ]
    }
   ],
   "source": [
    "# print(df.iloc[:, 2:7])\n",
    "X, Y = mod_def.create_lagged_features(df, n_lags=n_lags, shift_steps = shift_steps) \n",
    "print(f\"This is the shape of X\", X.shape)\n",
    "print(f\"This is the shape of Y\", Y.shape)\n",
    "\n",
    "# Train-test split (without shuffling to maintain temporal order)\n",
    "test_size = 0.3\n",
    "split_idx = int((1 - test_size) * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "Y_train, Y_test = Y[:split_idx], Y[split_idx:]\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVC model\n",
    "svc = SVC(class_weight='balanced')\n",
    "\n",
    "# Time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=2)  # Only 3 splits to reduce computation\n",
    "\n",
    "# Genetic Algorithm Search for best hyperparameters\n",
    "evolved_svc = GASearchCV(\n",
    "    estimator=svc,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    population_size=20,  # Reduce population size\n",
    "    generations=10,  # Reduce number of generations\n",
    "    n_jobs=-1,  # Use all cores\n",
    "    cv=tscv,  # Time series cross-validation\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "evolved_svc.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Best parameters from genetic algorithm\n",
    "print(\"Best Parameters found by GA:\", evolved_svc.best_params_)\n",
    "\n",
    "# Make predictions using the best found parameters\n",
    "Y_pred = evolved_svc.predict(X_test_scaled)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report with Best Parameters:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "# Plot true vs predicted\n",
    "plt.plot(Y_test, label=\"True\")\n",
    "plt.plot(Y_pred, label=\"Predicted\", linestyle='--')\n",
    "plt.title(\"True vs Predicted Binary Labels (3-Hour Forecast)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_c = confusion_matrix(Y_test, Y_pred)\n",
    "sns.heatmap(cm_c, annot=True, fmt='d').set_title(f\"Confusion matrix of SVM with a C value of {svc.C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classification report to a file\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "with open(\"/Users/tristan/Library/CloudStorage/OneDrive-StellenboschUniversity/Academics/Final_year/Semester_2/Skripsie/Report/images/results/reports/classification_report.txt\", \"a\") as file:\n",
    "    file.write(f\"\\nSVC Parameters:\\nC: {svc.C}\\nKernel: {svc.kernel}\\n\")\n",
    "    file.write(f\"Classification Report:\\n{report}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the DataFrame has a time index\n",
    "# if not isinstance(df.index, pd.DatetimeIndex):\n",
    "#     # Create a time index based on the sampling rate\n",
    "#     time_index = pd.date_range(start=0, periods=len(df), freq=f'{1/(1/12)}s')\n",
    "#     df.index = time_index\n",
    "\n",
    "\n",
    "\n",
    "# Perform the Fourier transform on each column\n",
    "# fft_results = {}\n",
    "# for column in df.columns[0:6]:\n",
    "#     print(column)\n",
    "#     ser = df[column].squeeze()\n",
    "#     print(ser)\n",
    "#     fft_data = fft(ser)\n",
    "#     # fft_results[column] = fft_data\n",
    "# print(\"GOt to this point!\")\n",
    "# # Create a DataFrame from the Fourier transform results\n",
    "# fft_df = pd.DataFrame(fft_results)\n",
    "\n",
    "# # Calculate the frequency axis based on the sampling rate and number of samples\n",
    "# frequency_axis = np.fft.fftfreq(len(df), 1/(1/12))\n",
    "\n",
    "# # Add the frequency axis as a new index\n",
    "# fft_df.index = frequency_axis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
